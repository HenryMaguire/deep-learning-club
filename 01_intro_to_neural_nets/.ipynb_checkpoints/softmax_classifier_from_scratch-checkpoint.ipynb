{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation from scratch\n",
    "\n",
    "## Multi-class Softmax classifier\n",
    "- Sigmoidal activation on the hidden-layer\n",
    "- Softmax output + Cross-Entropy loss\n",
    "- A single training example : no minibatches\n",
    "For more info on backpropagation of the loss function, see [this post](https://peterroelants.github.io/posts/cross-entropy-softmax/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_slope(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax of vector x in a numerically stable way.\"\"\"\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps/np.sum(exps, axis=0).reshape(1, -1)\n",
    "\n",
    "def softmax_grad(softmax):\n",
    "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "    s = softmax.reshape(-1,1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    return -np.mean(p*np.log2(q))\n",
    "\n",
    "def cross_entropy_slope(p, q):\n",
    "    return (-1/np.log(2))*p/q\n",
    "\n",
    "def cross_entropy_softmax_slope(p, y_labels):\n",
    "    \"\"\"\n",
    "    p : (num_classes, num_examples)\n",
    "    y_labels: (num_examples, n_classes)\n",
    "    \"\"\"\n",
    "    print(p.shape, y_labels.shape)\n",
    "    m = y_labels.shape[0]\n",
    "    grad = p.T\n",
    "    grad[range(m),y] -= 1\n",
    "    return (grad/m).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "m = nn.CrossEntropyLoss()\n",
    "#input = torch.Tensor(X) # n_classes, batchsize\n",
    "#output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7112) 1.000231016571742\n",
      "(2, 1) (1,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-349a44fba2a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m#print(w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxhline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxhline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_hidden = 20\n",
    "\n",
    "X = np.array([[0.25], \n",
    "              [0.25],  \n",
    "              [1]])   # (n_features, n_examples)\n",
    "\n",
    "# outputs are 1d\n",
    "y = np.array([1]) # (n_examples, n_classes)\n",
    "\n",
    "\n",
    "n_examples = y.shape[0]\n",
    "n_features = X.shape[0]\n",
    "n_output = 2\n",
    "np.random.seed(1)\n",
    "learning_rate = 0.1\n",
    "batchsize = y.shape[0]\n",
    "\n",
    "W1 = np.random.normal(size=(n_hidden, n_features)) # assume some random weights and data\n",
    "W2 = np.random.normal(size=(n_output, n_hidden)) # assume some random weights and data\n",
    "\n",
    "losses = []\n",
    "outputs = []\n",
    "\n",
    "for j in range(300):\n",
    "    s1 = np.dot(W1, X)  # (n_hidden, batchsize)\n",
    "    f1 = sigmoid(s1)    # ... again\n",
    "    assert f1.shape == (n_hidden, batchsize)\n",
    "    \n",
    "    # output of second linear layer\n",
    "    s2 = np.dot(W2, f1) # (n_classes, batchsize)\n",
    "    f2 = softmax(s2)    # ... again\n",
    "    \n",
    "    assert f2.shape == (2, batchsize)\n",
    "    # least squares loss\n",
    "    L = cross_entropy(y, f2) \n",
    "    print(output, L)\n",
    "    # backward pass \n",
    "    #dLdf2 = cross_entropy_slope(y, f2.T)  # dimensions of f2\n",
    "    #df2ds2 = softmax_grad(f2) # ()\n",
    "    \n",
    "    #dLds2 = dLdf2.dot(df2ds2)\n",
    "    dLds2 = cross_entropy_softmax_slope(f2, y) # (n_classes, batchsize)\n",
    "    assert dLds2.shape == (2, 1), dLds2.shape\n",
    "    dLdW2 = f1.dot(dLds2.T).T # n_output, n_hidden\n",
    "    assert dLdW2.shape == (n_output, n_hidden), dLdW2.shape\n",
    "    ds2df1 = W2\n",
    "    dLdf1 = ds2df1.T.dot(dLds2) # (n_hidden, batchsize)\n",
    "    assert dLdf1.shape == (n_hidden, batchsize), dLdf1.shape\n",
    "    \n",
    "    dLds1 = sigmoid_slope(f1)\n",
    "    dLdW1 = X.dot(dLds1.T).T  # (n_hidden, n_features)\n",
    "    \n",
    "    assert dLdW1.shape == (n_hidden, n_features)\n",
    "    #print(W2.shape, learning_rate, dLdW2)\n",
    "    W2 += -learning_rate*dLdW2\n",
    "    W1 += -learning_rate*dLdW1\n",
    "    \n",
    "    losses.append(L)\n",
    "    outputs.append(f2)\n",
    "    break\n",
    "outputs = np.array(outputs).reshape(-1, 2)\n",
    "#print(outputs)\n",
    "fig, axs = plt.subplots(2, 1, sharex=True, figsize=(10,6))\n",
    "axs[0].set_ylabel('Output')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "\n",
    "#print(w)\n",
    "axs[0].axhline(y[0][0], ls='--')\n",
    "axs[0].axhline(y[0][1], ls='--')\n",
    "axs[0].plot(outputs[:, 0])\n",
    "axs[0].plot(outputs[:, 1])\n",
    "axs[1].plot(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem to converge, but they are quite unstable (try turning up the number of epochs)\n",
    "\n",
    "# Two layer softmax classifier, with minibatches\n",
    "\n",
    "Here is where things start to break down a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (2, 3, 6) (2, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,2) and (2,3,6) not aligned: 2 (dim 1) != 3 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-762c078d6e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdf2ds2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2ds2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2ds2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdLds2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdLdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2ds2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mdLdW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdLds2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m# n_output, n_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,2) and (2,3,6) not aligned: 2 (dim 1) != 3 (dim 1)"
     ]
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "\n",
    "\n",
    "X = np.array([[0.25, -0.25, -0.3], \n",
    "              [0.25, -0.25, -0.32],  \n",
    "              [1,    -1,    -1.05]])   # (n_features, n_examples)\n",
    "\n",
    "# outputs are 1d\n",
    "y = np.array([[1, 0, 0],\n",
    "              [0, 1, 1]])\n",
    "\n",
    "n_examples = y.shape[0]\n",
    "n_features = X.shape[0]\n",
    "n_output = 2\n",
    "np.random.seed(1)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "W1 = np.random.normal(size=(n_hidden, n_features)) # assume some random weights and data\n",
    "W2 = np.random.normal(size=(n_output, n_hidden)) # assume some random weights and data\n",
    "\n",
    "losses = []\n",
    "outputs = []\n",
    "\n",
    "for j in range(300):\n",
    "    s1 = np.dot(W1, X)  # (n_hidden, batchsize)\n",
    "    f1 = sigmoid(s1)    # ... again\n",
    "    \n",
    "    # output of second linear layer\n",
    "    s2 = np.dot(W2, f1) # (n_output (2), batchsize)\n",
    "    f2 = softmax(s2)    # ... again\n",
    "    \n",
    "    # least squares loss\n",
    "    L = cross_entropy(y, f2)\n",
    "    \n",
    "    dLdf2 = cross_entropy_slope(y, f2) # (1, 2)\n",
    "    \n",
    "    df2ds2 = softmax_grad(f2)\n",
    "    print(f2.shape, df2ds2.shape, dLdf2.shape)\n",
    "    \n",
    "    #df2ds2 = df2ds2.reshape(f2.shape[0], f2.shape[1], -1) ?\n",
    "\n",
    "    dLds2 = dLdf2.T.dot(df2ds2)\n",
    "    dLdW2 = f1.dot(dLds2).T # n_output, n_hidden\n",
    "    \n",
    "    ds2df1 = W2\n",
    "    dLdf1 = dLds2.dot(ds2df1) # (n_hidden, batchsize)\n",
    "    \n",
    "    \n",
    "    dLds1 = sigmoid_slope(f1)\n",
    "    dLdW1 = X.dot(dLds1.T)  # (n_hidden, n_features)\n",
    "    \n",
    "    W2 += -learning_rate*dLdW2\n",
    "    W1 += -learning_rate*dLdW1.T\n",
    "    \n",
    "    losses.append(L)\n",
    "    outputs.append(f2)\n",
    "    \n",
    "outputs = np.array(outputs).reshape(-1, 2)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex=True, figsize=(10,6))\n",
    "axs[0].set_ylabel('Output')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "\n",
    "\n",
    "axs[0].axhline(y[0][0], ls='--')\n",
    "axs[0].axhline(y[0][1], ls='--')\n",
    "axs[0].plot(outputs[:, 0])\n",
    "axs[0].plot(outputs[:, 1])\n",
    "axs[1].plot(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can't figure out how to make the local derivative $$\\partial f_2/ \\partial s_2$$\n",
    "have the right dimensions. I guess I have to reshape some of the matrices somewhere?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
